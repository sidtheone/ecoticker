# US-1.1 Functional Validation Plan

**Date:** 2026-02-13
**User Story:** US-1.1 — Multi-Dimensional Scoring
**Purpose:** End-to-end functional validation using real API keys
**Prerequisites:** PostgreSQL running, NEWSAPI_KEY + OPENROUTER_API_KEY configured

---

## Validation Objectives

1. ✅ Verify batch pipeline runs end-to-end with real LLM
2. ✅ Confirm sub-scores (health, eco, econ) + levels stored correctly
3. ✅ Validate server-side aggregation (weighted average)
4. ✅ Test anomaly detection with historical data
5. ✅ Verify seed data creates all severity levels + INSUFFICIENT_DATA
6. ✅ Confirm GDPR audit log purge executes
7. ✅ Validate UI displays new fields (scoreReasoning visible in API)

---

## Phase 1: Environment Setup

### 1.1 — PostgreSQL Database

```bash
# Start PostgreSQL (Docker)
docker compose up -d postgres

# Wait for readiness
docker compose logs postgres | grep "ready to accept connections"

# Push schema
npx drizzle-kit push

# Verify schema
npx drizzle-kit studio
# → Check that score_history table has:
#    - health_level, eco_level, econ_level (text)
#    - health_reasoning, eco_reasoning, econ_reasoning (text, nullable)
#    - overall_summary, raw_llm_response, anomaly_detected
```

**Expected:** Schema matches `src/db/schema.ts` exactly.

---

### 1.2 — Environment Variables

Create `.env.local` (gitignored):

```bash
# Database
DATABASE_URL=postgresql://ecoticker:password@localhost:5432/ecoticker

# NewsAPI (free tier: 100 requests/day)
NEWSAPI_KEY=your_newsapi_key_here

# OpenRouter (pay-as-you-go)
OPENROUTER_API_KEY=your_openrouter_key_here
OPENROUTER_MODEL=meta-llama/llama-3.1-8b-instruct:free

# Batch keywords (avoid over-fetching)
BATCH_KEYWORDS=climate change,deforestation,air quality

# Admin (for protected endpoints)
ADMIN_API_KEY=test-admin-key-123
```

**Cost estimate:**
- NewsAPI: Free (3 keywords = 1 request)
- OpenRouter: ~$0.05 per batch run (classification + 3 topics × scoring)

---

## Phase 2: Seed Data Validation

### 2.1 — Run Seed Script

```bash
npx tsx scripts/seed.ts
```

**Expected output:**
```
=== EcoTicker Seed Script v2 ===
Time: 2026-02-13T...

[1/5] Clearing existing data...
✓ Database cleared

[2/5] Inserting topics...
  ✓ Amazon Deforestation Acceleration (overall=66, urgency=critical)
  ✓ Delhi Air Quality Emergency (overall=58, urgency=moderate)
  ...
  ✓ Deep Sea Mining Exploration Permits (overall=47, urgency=moderate)
✓ Inserted 13 topics

[3/5] Inserting articles...
✓ Inserted 39 articles

[4/5] Inserting score history...
✓ Inserted 91 score history entries

[5/5] Inserting keywords...
✓ Inserted 65 keywords

=== Seed Complete ===
Topics: 13
Articles: 39
Score history: 91
Keywords: 65

✓ All severity levels represented
✓ INSUFFICIENT_DATA example included (Deep Sea Mining - econ dimension)
✓ Realistic reasoning text based on real-world scenarios
```

### 2.2 — Database Verification (Drizzle Studio)

```bash
npx drizzle-kit studio
```

**Manual checks:**

| Table | Verification |
|-------|-------------|
| **topics** | • 13 rows <br> • healthScore, ecoScore, econScore populated <br> • scoreReasoning present <br> • At least 1 row with econScore = null (Deep Sea Mining) |
| **articles** | • 39 rows (3 per topic) <br> • Titles reference topic names |
| **score_history** | • 91 rows (7 per topic) <br> • healthLevel, ecoLevel, econLevel populated <br> • Reasoning fields populated (150 chars max) <br> • At least 7 entries with econLevel = "INSUFFICIENT_DATA" |
| **topic_keywords** | • 65+ rows <br> • Keywords lowercase |

### 2.3 — API Endpoint Verification

```bash
# Start dev server
npm run dev

# Test topics endpoint (includes sub-scores)
curl http://localhost:3000/api/topics | jq '.topics[0] | {name, healthScore, ecoScore, econScore, urgency}'

# Test topic detail (includes reasoning)
curl http://localhost:3000/api/topics/amazon-deforestation-acceleration | jq '{topic: .topic.name, scoreReasoning: .topic.scoreReasoning, scoreHistory: .scoreHistory | length}'
```

**Expected:**
- `/api/topics` returns healthScore, ecoScore, econScore for all topics
- `/api/topics/[slug]` returns scoreHistory with levels + reasoning

---

## Phase 3: Batch Pipeline Validation

### 3.1 — Pre-Batch State

```bash
# Get current topic count
psql $DATABASE_URL -c "SELECT COUNT(*) FROM topics;"
# → Should be 13 (from seed)

# Get baseline score for a topic (we'll check for anomaly detection)
psql $DATABASE_URL -c "SELECT name, health_score, eco_score, econ_score FROM topics WHERE slug = 'amazon-deforestation-acceleration';"
# → Record these values
```

### 3.2 — Run Batch Pipeline

```bash
npx tsx scripts/batch.ts
```

**Expected console output:**

```
=== EcoTicker Batch Pipeline v2 (US-1.1) ===
Time: 2026-02-13T...

[1/4] Fetching news...
Fetched 15 articles

[2/4] Classifying articles into topics...
Classified into 3 topics

[3/4] Scoring topics...
  Climate Crisis Updates: overall=72, urgency=critical
  Amazon Fires Intensify: overall=85, urgency=breaking ⚠️ ANOMALY
  Renewable Energy Milestone: overall=18, urgency=informational

⚠️  WARNING: 8.3% of dimension scores were clamped. Possible model drift. Review LLM responses.

[GDPR] Purging old audit logs...
Purged 0 audit logs older than 90 days

[4/4] Done! 16 topics, 54 articles in database.
```

**Key observations:**
- Classification works (groups articles into topics)
- Scoring produces sub-scores
- Anomaly detection triggers for significant jumps
- Batch-level clamping warning appears if >30% clamped
- GDPR purge executes (even if 0 rows)

### 3.3 — Post-Batch Database Verification

```bash
# Check that new topics were created
psql $DATABASE_URL -c "SELECT COUNT(*) FROM topics;"
# → Should be 16 (13 seed + 3 new from batch)

# Inspect a new topic's scores
psql $DATABASE_URL -c "SELECT name, current_score, health_score, eco_score, econ_score, health_level, eco_level, econ_level FROM topics WHERE name LIKE 'Climate%';"

# Check score history was created
psql $DATABASE_URL -c "SELECT COUNT(*) FROM score_history WHERE topic_id = (SELECT id FROM topics WHERE name LIKE 'Climate%');"
# → Should be 1 (one history entry per batch run)

# Verify anomaly detection flag
psql $DATABASE_URL -c "SELECT topic_id, anomaly_detected FROM score_history WHERE anomaly_detected = TRUE;"
# → Should show entries for topics that jumped >25pts
```

### 3.4 — LLM Response Validation

Open Drizzle Studio and inspect `score_history` table:

```bash
npx drizzle-kit studio
```

**Manual checks:**
1. **rawLlmResponse:** Contains full JSON from LLM (including healthReasoning, ecoReasoning, econReasoning)
2. **Reasoning fields:** Populated with 2-3 sentences citing articles
3. **Levels:** Match the scores (e.g., SEVERE = 76-100)
4. **overallSummary:** 1-2 sentence synthesis

**Example validation query:**
```sql
SELECT
  topic_id,
  health_score,
  health_level,
  LENGTH(health_reasoning) as reasoning_length,
  anomaly_detected
FROM score_history
WHERE health_level != 'INSUFFICIENT_DATA'
ORDER BY recorded_at DESC
LIMIT 5;
```

Expected: All reasoning lengths > 50 chars (meaningful text, not "null" or empty).

---

## Phase 4: Scoring Function Unit Tests

```bash
# Run scoring tests
npx jest tests/scoring.test.ts

# Expected: All 31 tests pass
```

**Coverage check:**
```bash
npx jest --coverage tests/scoring.test.ts
```

Expected: 100% coverage for `src/lib/scoring.ts`.

---

## Phase 5: Integration Test Validation

```bash
# Run all tests (skips some due to missing updates — see Phase 6)
npx jest

# Expected: Some failures in batch.test.ts and seed.test.ts (old v1 tests)
# Focus: scoring.test.ts should pass 100%
```

---

## Phase 6: Known Test Gaps (Deferred to Chunk 2 completion)

The following tests need updates but are not blocking functional validation:

- `tests/batch.test.ts`: Update for `processScoreResult()` integration
- `tests/seed.test.ts`: Update for v2 seed data verification
- Component tests: Update for `scoreReasoning` display (US-1.2)

**Action:** Mark as TODO, implement after functional validation passes.

---

## Phase 7: API Response Validation

### 7.1 — Topics Endpoint

```bash
curl -s http://localhost:3000/api/topics | jq '
  .topics[] |
  select(.slug == "amazon-deforestation-acceleration") |
  {
    name,
    currentScore,
    healthScore,
    ecoScore,
    econScore,
    urgency,
    scoreReasoning
  }
'
```

**Expected:**
```json
{
  "name": "Amazon Deforestation Acceleration",
  "currentScore": 66,
  "healthScore": 28,
  "ecoScore": 88,
  "econScore": 62,
  "urgency": "critical",
  "scoreReasoning": "Unprecedented Amazon deforestation rate threatens..."
}
```

### 7.2 — Topic Detail Endpoint

```bash
curl -s http://localhost:3000/api/topics/amazon-deforestation-acceleration | jq '
  {
    topic: .topic.name,
    scoreHistory: [
      .scoreHistory[] |
      {
        score,
        healthLevel,
        ecoLevel,
        econLevel,
        healthReasoning: (.healthReasoning // "N/A" | .[0:50]),
        anomalyDetected
      }
    ] | .[0:2]
  }
'
```

**Expected:**
- scoreHistory array with levels and abbreviated reasoning
- At least 7 entries (from seed) + new entries from batch
- Anomaly flag visible for topics that jumped

---

## Phase 8: Anomaly Detection Real-World Test

### 8.1 — Simulate Anomaly

```sql
-- Manually insert a topic with low score
INSERT INTO topics (name, slug, category, region, current_score, health_score, eco_score, econ_score, urgency, impact_summary, article_count)
VALUES ('Test Anomaly Topic', 'test-anomaly', 'climate', 'Global', 30, 30, 30, 30, 'moderate', 'Test summary', 0);
```

### 8.2 — Run Batch with Anomaly

Modify BATCH_KEYWORDS to include a term that will update "Test Anomaly Topic" (e.g., add it to the classification manually in batch.ts temporarily).

Expected batch output:
```
  Test Anomaly Topic: overall=85, urgency=breaking ⚠️ ANOMALY
WARNING: Test Anomaly Topic eco score jumped 55 points (30 → 85). Manual review recommended.
```

---

## Phase 9: GDPR Audit Log Purge Test

### 9.1 — Create Old Audit Logs

```sql
-- Insert audit logs from 100 days ago
INSERT INTO audit_logs (timestamp, ip_address, endpoint, method, action, success, details)
VALUES
  (NOW() - INTERVAL '100 days', '192.168.1.1', '/api/seed', 'POST', 'seed', TRUE, '{"topics": 12}'),
  (NOW() - INTERVAL '95 days', '192.168.1.2', '/api/batch', 'POST', 'batch', TRUE, '{"articles": 20}');

-- Verify they exist
SELECT COUNT(*) FROM audit_logs WHERE timestamp < NOW() - INTERVAL '90 days';
-- → Should be 2
```

### 9.2 — Run Batch (Triggers Purge)

```bash
npx tsx scripts/batch.ts
```

Expected output:
```
[GDPR] Purging old audit logs...
Purged 2 audit logs older than 90 days
```

### 9.3 — Verify Purge

```sql
SELECT COUNT(*) FROM audit_logs WHERE timestamp < NOW() - INTERVAL '90 days';
-- → Should be 0
```

---

## Phase 10: Cost & Performance Metrics

### 10.1 — Batch Pipeline Performance

Run batch 3 times and record:

```bash
time npx tsx scripts/batch.ts
```

**Metrics to track:**
- Total runtime (target: <2min for 3 topics)
- LLM latency (classification + scoring)
- Database insert time
- Memory usage

### 10.2 — OpenRouter Cost

Check OpenRouter dashboard after 3 batch runs:

- Classification: ~1,000 tokens per call
- Scoring: ~1,500 tokens per topic (with few-shot examples)
- Total: ~6,500 tokens per batch = $0.03-0.10 depending on model

---

## Validation Checklist

| Phase | Test | Status | Notes |
|-------|------|--------|-------|
| **2** | Seed creates 13 topics | ⏳ | Run `npx tsx scripts/seed.ts` |
| **2** | INSUFFICIENT_DATA present | ⏳ | Check Deep Sea Mining econ dimension |
| **2** | Severity levels cover full range | ⏳ | MINIMAL, MODERATE, SIGNIFICANT, SEVERE all present |
| **3** | Batch fetches news | ⏳ | NewsAPI returns articles |
| **3** | LLM classification works | ⏳ | Articles grouped into topics |
| **3** | LLM scoring returns sub-scores | ⏳ | health, eco, econ all populated |
| **3** | Server-side validation clamps scores | ⏳ | Check for clamping warnings |
| **3** | Server-side aggregation correct | ⏳ | Overall = weighted average |
| **3** | Anomaly detection triggers | ⏳ | >25pt jump logs warning |
| **3** | GDPR purge executes | ⏳ | Old audit logs deleted |
| **4** | Scoring unit tests pass | ⏳ | 31/31 tests green |
| **7** | API returns sub-scores | ⏳ | /api/topics includes healthScore, ecoScore, econScore |
| **7** | API returns reasoning | ⏳ | /api/topics/[slug] includes scoreReasoning |

---

## Success Criteria

✅ **Functional validation PASSES when:**

1. Seed script creates all data with levels + reasoning
2. Batch pipeline scores 3+ real topics with LLM
3. Sub-scores (health, eco, econ) stored correctly
4. Server-side aggregation matches expected formula
5. Anomaly detection logs warnings for >25pt jumps
6. GDPR audit log purge removes old entries
7. API endpoints return new v2 fields
8. All scoring unit tests pass (31/31)

---

## Troubleshooting

### Issue: NewsAPI returns 0 articles

**Cause:** Free tier rate limit (100/day) exceeded or keywords too specific.

**Fix:**
```bash
# Use broader keywords
export BATCH_KEYWORDS="climate,pollution,environment"
```

---

### Issue: LLM returns invalid JSON

**Cause:** Model doesn't respect `response_format: json_object`.

**Fix:**
```typescript
// batch.ts already has extractJSON() fallback
// Check rawLlmResponse in Drizzle Studio to see actual response
```

---

### Issue: All scores clamped

**Cause:** LLM assigning scores outside level ranges.

**Fix:**
- Review few-shot examples in batch.ts
- Try different model (e.g., `meta-llama/llama-3.1-70b-instruct`)
- Check prompt clarity

---

### Issue: No anomaly detected despite large jump

**Cause:** Topic doesn't have previous scores (new topic).

**Fix:** Expected behavior — anomalies only detected for existing topics.

---

## Next Steps After Validation

1. ✅ Mark US-1.1 as COMPLETE
2. → Update memory with validation results
3. → Commit with message: `feat(scoring): complete US-1.1 validation (batch + seed + tests)`
4. → Begin US-1.2 (UI for sub-scores and reasoning display)
